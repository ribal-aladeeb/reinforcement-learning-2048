batch_size = 5000  # number of experiences to sample
discount_factor = 0.95  # used in q-learning equation (Bellman equation)
target_model = copy.deepcopy(model)
replay_buffer = deque(maxlen=15000)  # [(state, action, reward, next_state, done),...]
learning_rate = 1e-4  # optimizer for gradient descent
loss_fn = nn.MSELoss(reduction='sum')
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
no_episodes = 50000
no_episodes_to_reach_epsilon = 10000
min_epsilon = 0.01
no_episodes_before_training = 2000
no_episodes_before_updating_target = 100
use_double_dqn = True

batch_size = 5000  # number of experiences to sample
discount_factor = 0.95  # used in q-learning equation (Bellman equation)
target_model = copy.deepcopy(model)
replay_buffer = deque(maxlen=15000)  # [(state, action, reward, next_state, done),...]
learning_rate = 1e-4  # optimizer for gradient descent
loss_fn = nn.MSELoss(reduction='sum')
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
no_episodes = 50000
no_episodes_to_reach_epsilon = 10000
min_epsilon = 0.01
no_episodes_before_training = 2000
no_episodes_before_updating_target = 100
use_double_dqn = False

batch_size = 10000  # number of experiences to sample
discount_factor = 0.95  # used in q-learning equation (Bellman equation)
target_model = copy.deepcopy(model)
replay_buffer = deque(maxlen=50000)  # [(state, action, reward, next_state, done),...]
learning_rate = 1e-4  # optimizer for gradient descent
loss_fn = nn.MSELoss(reduction='sum')
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
no_episodes = 50000
no_episodes_to_reach_epsilon = 10000
min_epsilon = 0.01
no_episodes_before_training = 2000
no_episodes_before_updating_target = 100
use_double_dqn = True

batch_size = 5000  # number of experiences to sample
discount_factor = 0.95  # used in q-learning equation (Bellman equation)
target_model = copy.deepcopy(model)
replay_buffer = deque(maxlen=15000)  # [(state, action, reward, next_state, done),...]
learning_rate = 1e-4  # optimizer for gradient descent
loss_fn = nn.MSELoss(reduction='sum')
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
no_episodes = 50000
no_episodes_to_reach_epsilon = 10000
min_epsilon = 0.01
no_episodes_before_training = 2000
no_episodes_before_updating_target = 10
use_double_dqn = True

batch_size = 5000  # number of experiences to sample
discount_factor = 0.80  # used in q-learning equation (Bellman equation)
target_model = copy.deepcopy(model)
replay_buffer = deque(maxlen=15000)  # [(state, action, reward, next_state, done),...]
learning_rate = 1e-4  # optimizer for gradient descent
loss_fn = nn.MSELoss(reduction='sum')
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
no_episodes = 50000
no_episodes_to_reach_epsilon = 10000
min_epsilon = 0.01
no_episodes_before_training = 2000
no_episodes_before_updating_target = 100
use_double_dqn = True
